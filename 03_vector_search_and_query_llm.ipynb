{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Conduct a vector similarity search with the Prompt and Send the Final Prompt to the LLM\n",
    "\n",
    "In this notebook, we will first conduct a similarity search in the ChromaDB for texts which exhibit the least distance from the prompt created in step 02. 10 pieces of text will selected, but this number can be changed as desired. \n",
    "\n",
    "Subsequently, the final prompt (which includes both the prompt and the found results) will be sent to an LLM on AWS Bedrock. \n",
    "\n",
    "> Credit for parts of this notebook: https://github.com/Farzad-R/Advanced-QA-and-RAG-Series/commits?author=Farzad-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from pyprojroot import here\n",
    "\n",
    "# Load ChromaDB\n",
    "chroma_client = chromadb.PersistentClient(path=str(here(\"data/chroma\")))\n",
    "vectordb = chroma_client.get_collection(name=\"renalworks\")\n",
    "vectordb.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from src.utils.bedrock_caller import BedrockCaller\n",
    "\n",
    "bedrock_caller = BedrockCaller()\n",
    "\n",
    "# Specify the path of the prompt\n",
    "prompt_file_name = 'clinical-notes-01-prompt-no-sample.txt'\n",
    "prompt_file_path = os.path.join('prompts', prompt_file_name)\n",
    "\n",
    "# Load prompt\n",
    "with open(prompt_file_path, 'r') as f:\n",
    "    prompt = f.read()\n",
    "\n",
    "# Obtain prompt embedding\n",
    "prompt_embedding = bedrock_caller.get_embedding(prompt)\n",
    "print(prompt_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct a similarity search in the ChromaDB\n",
    "similarity_search_results = vectordb.query(\n",
    "    query_embeddings = prompt_embedding,\n",
    "    n_results=20 # Adjust the number of results to obtain as required \n",
    ")['documents']\n",
    "\n",
    "print(similarity_search_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction of Final Prompt & Response Generation\n",
    "\n",
    "The final prompt is constructed as with the prompt from step 02 and the similarity search results obtained from above, before being sent to Anthropic's Claude 3 Sonnet for response generation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file_name = 'clinical-notes-01.txt'\n",
    "sample_file_path = os.path.join('sample_responses', sample_file_name)\n",
    "\n",
    "# Load sample\n",
    "with open(sample_file_path, 'r') as f:\n",
    "    sample = f.read()\n",
    "\n",
    "system_role = \"You are a highly knowledgeable assistant specialized in creating clinical documents and answering questions. You will receive the user's question along with the search results of that question over a database. Please use the search results and follow the sample's format to generate the proper answer. Please use JSON format. \"\n",
    "final_prompt = f\"User's question: {prompt} \\n\\n Search results:\\n {similarity_search_results} \\n\\n Sample: {sample}\"\n",
    "\n",
    "messages_API_body = {\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\", \n",
    "    \"max_tokens\": int(500/0.75),\n",
    "    \"system\": system_role,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": final_prompt\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_body = bedrock_caller.call_claude3sonnet(messages_API_body)\n",
    "llm_output = response_body['content'][0]['text']\n",
    "\n",
    "print(llm_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LLM output is then saved into the `results` folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "sample_file_name = 'clinical-notes-01.txt'\n",
    "save_dir = os.path.join('results', os.path.splitext(sample_file_name)[0])\n",
    "\n",
    "# Regular expression to match files named 'attempt-x.txt'\n",
    "pattern = re.compile(r'attempt-(\\d+)\\.txt')\n",
    "max_num = -1\n",
    "\n",
    "# Find the highest attempt number\n",
    "for filename in os.listdir(save_dir):\n",
    "    match = pattern.match(filename)\n",
    "    if match:\n",
    "        num = int(match.group(1))\n",
    "        if num > max_num:\n",
    "            max_num = num\n",
    "next_num = max_num + 1 if max_num != -1 else 1\n",
    "\n",
    "# Create the new filename\n",
    "new_filename = f'attempt-{next_num}.txt'\n",
    "new_filepath = os.path.join(save_dir, new_filename)\n",
    "\n",
    "# Save the data to the new file\n",
    "with open(new_filepath, 'w') as file:\n",
    "    file.write(llm_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
